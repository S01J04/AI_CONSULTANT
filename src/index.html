
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advisor Chatbot with Voice</title>
    <style>
        /* ... (keep existing styles) ... */
    </style>
</head>
<body>
    <h1>Advisor Chatbot with Voice</h1>
    
    <div class="tabs">
        <button class="tab active" onclick="openTab(event, 'voice-to-voice')">Voice to Voice</button>
    </div>
    
    <div class="controls">
        <div>
            <label for="user-id">User ID:</label>
            <input type="text" id="user-id" value="user123">
        </div>
    </div>
    
    <div class="chat-container" id="chat-container"></div>
    
    <div id="voice-to-voice" class="tab-content active">
        <div class="voice-controls">
            <button id="start-voice-to-voice">Start Voice Chat</button>
            <button id="stop-voice-to-voice" disabled>Stop Voice Chat</button>
        </div>
        <div class="status-indicator" id="voice-to-voice-status">Ready for voice conversation</div>
    </div>

    <script>
        const API_URL = 'https://consultant-bot-791977318929.us-central1.run.app';
        let audioChunks = [];
        let mediaRecorder = null;
        let socket = null;
        let isRecording = false;
        let audioContext = null;
        
        const chatContainer = document.getElementById('chat-container');
        const userIdInput = document.getElementById('user-id');
        const startVoiceToVoiceBtn = document.getElementById('start-voice-to-voice');
        const stopVoiceToVoiceBtn = document.getElementById('stop-voice-to-voice');
        const voiceToVoiceStatus = document.getElementById('voice-to-voice-status');
        
        startVoiceToVoiceBtn.addEventListener('click', startVoiceToVoiceChat);
        stopVoiceToVoiceBtn.addEventListener('click', stopVoiceToVoiceChat);
        
        function checkBrowserSupport() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                alert("Your browser doesn't support audio recording. Please use Chrome, Firefox, or Edge.");
                disableVoiceButtons();
                return false;
            }
            
            if (!window.WebSocket) {
                alert("Your browser doesn't support WebSockets. Please use Chrome, Firefox, or Edge.");
                disableVoiceButtons();
                return false;
            }
            
            return true;
        }
        
        function disableVoiceButtons() {
            startVoiceToVoiceBtn.disabled = true;
            voiceToVoiceStatus.textContent = "Voice features not supported by your browser";
        }
        
        function addMessageToChat(sender, message) {
            const messageElement = document.createElement('div');
            messageElement.className = sender === 'user' ? 'user-message' : 'bot-message';
            messageElement.textContent = message;
            chatContainer.appendChild(messageElement);
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }
        
        async function startVoiceToVoiceChat() {
            if (!checkBrowserSupport()) return;
            
            const userId = userIdInput.value.trim();
            if (!userId) {
                voiceToVoiceStatus.textContent = 'Please enter a User ID';
                return;
            }
            
            voiceToVoiceStatus.textContent = 'Connecting...';
            
            try {
                socket = new WebSocket(`wss://${API_URL.replace(/^https?:\/\//, '')}/ws/voice/${userId}`);
                
                socket.onopen = () => {
                    voiceToVoiceStatus.textContent = 'Connected. Ready to start voice chat.';
                    startVoiceToVoiceBtn.disabled = true;
                    stopVoiceToVoiceBtn.disabled = false;
                    startVoiceRecording();
                };
                
                socket.onmessage = async (event) => {
                    if (event.data instanceof Blob) {
                        playAudioResponse(event.data);
                    } else {
                        try {
                            const data = JSON.parse(event.data);
                            
                            if (data.type === 'transcript') {
                                addMessageToChat('user', data.text);
                            } else if (data.type === 'text_response') {
                                addMessageToChat('bot', data.text);
                                speakText(data.text);
                            } else if (data.type === 'end_of_audio') {
                                voiceToVoiceStatus.textContent = 'Ready for next voice input';
                                if (socket && socket.readyState === WebSocket.OPEN) {
                                    startVoiceRecording();
                                }
                            } else if (data.type === 'error') {
                                voiceToVoiceStatus.textContent = `Error: ${data.message}`;
                            }
                        } catch (error) {
                            console.error('Error parsing WebSocket message:', error);
                        }
                    }
                };
                
                socket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    voiceToVoiceStatus.textContent = 'Connection error';
                    stopVoiceToVoiceChat();
                };
                
                socket.onclose = () => {
                    voiceToVoiceStatus.textContent = 'Connection closed';
                    startVoiceToVoiceBtn.disabled = false;
                    stopVoiceToVoiceBtn.disabled = true;
                };
                
            } catch (error) {
                console.error('Error starting voice chat:', error);
                voiceToVoiceStatus.textContent = `Error: ${error.message}`;
            }
        }
        
        async function startVoiceRecording() {
            audioChunks = [];
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                        
                        if (socket && socket.readyState === WebSocket.OPEN) {
                            socket.send(event.data);
                        }
                    }
                };
                
                mediaRecorder.onstart = () => {
                    isRecording = true;
                    voiceToVoiceStatus.textContent = 'Recording... (speak now)';
                    startVoiceToVoiceBtn.classList.add('recording');
                };
                
                mediaRecorder.onstop = () => {
                    isRecording = false;
                    voiceToVoiceStatus.textContent = 'Processing audio...';
                    startVoiceToVoiceBtn.classList.remove('recording');
                    
                    if (socket && socket.readyState === WebSocket.OPEN) {
                        socket.send(new Blob([new Uint8Array([69, 78, 68, 95, 79, 70, 95, 65, 85, 68, 73, 79])], { type: 'application/octet-stream' }));
                    }
                    
                    stream.getTracks().forEach(track => track.stop());
                };
                
                mediaRecorder.start(100);
                
                setTimeout(() => {
                    if (mediaRecorder && mediaRecorder.state === 'recording') {
                        mediaRecorder.stop();
                    }
                }, 10000);
                
            } catch (error) {
                console.error('Error starting recording:', error);
                voiceToVoiceStatus.textContent = `Error: ${error.message}`;
            }
        }
        
        function stopVoiceToVoiceChat() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
            }
            
            if (socket) {
                socket.close();
                socket = null;
            }
            
            startVoiceToVoiceBtn.disabled = false;
            stopVoiceToVoiceBtn.disabled = true;
            startVoiceToVoiceBtn.classList.remove('recording');
            voiceToVoiceStatus.textContent = 'Ready for voice conversation';
        }
        
        function speakText(text) {
            if ('speechSynthesis' in window) {
                const cleanText = text
                    .replace(/#{1,6}\s*/g, '')
                    .replace(/\*\*/g, '')
                    .replace(/\*/g, '')
                    .replace(/_/g, '')
                    .replace(/`/g, '')
                    .replace(/\n\s*[-*+]\s/g, '. ')
                    .replace(/\n/g, '. ')
                    .replace(/\s{2,}/g, ' ')
                    .replace(/[^\w\s.,!?]/g, '');
        
                const utterance = new SpeechSynthesisUtterance(cleanText);
                speechSynthesis.speak(utterance);
            } else {
                console.error('Speech synthesis not supported by your browser');
            }
        }
        
        async function playAudioResponse(audioBlob) {
            try {
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)({
                        sampleRate: 44100
                    });
                }
                
                const audioURL = URL.createObjectURL(audioBlob);
                
                const response = await fetch(audioURL);
                const arrayBuffer = await response.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                source.start(0);
                
                voiceToVoiceStatus.textContent = 'Playing audio response...';
                
                source.onended = () => {
                    URL.revokeObjectURL(audioURL);
                    voiceToVoiceStatus.textContent = 'Ready for next voice input';
                };
                
            } catch (error) {
                console.error('Error in audio playback:', error);
                voiceToVoiceStatus.textContent = 'Audio playback failed';
            }
        }
        
        window.onload = function() {
            checkBrowserSupport();
        };
    </script>
</body>
</html>

